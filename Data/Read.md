# ğŸ“‚ Data Folder

## ğŸ“Œ Overview
This folder contains the dataset for the **Alzheimer's Detection Model**.  
The dataset consists of **6,460 grayscale MRI brain scans** of size **128Ã—128** pixels, used to classify Alzheimer's disease.

Due to storage limitations, only the **raw dataset** is included here. The **augmented and preprocessed** data is not uploaded but can be generated by following the preprocessing steps in the provided scripts.

---

## ğŸŒ Dataset Source
You can access the original dataset from the following source:  

ğŸ“Œ **[Dataset Link](https://www.kaggle.com/datasets/geethashashankp/alzheimers-disease-mri-dataset/data)**  

Please refer to the dataset license and terms of use before using it.

---

## ğŸ“‚ Folder Structure
ğŸ“‚ data/ â”œâ”€â”€ raw/ # Original dataset (uploaded) â”‚ â”œâ”€â”€ class_0/ # MRI scans of healthy brains â”‚ â”œâ”€â”€ class_1/ # MRI scans of early-stage Alzheimer's â”‚ â”œâ”€â”€ class_2/ # MRI scans of moderate Alzheimer's â”‚ â”œâ”€â”€ class_3/ # MRI scans of severe Alzheimer's â”‚ â”œâ”€â”€ processed/ # Preprocessed and augmented data (not uploaded)

## ğŸ”¹ Dataset Details
- **Total Images**: 6,460  
- **Image Size**: 128 Ã— 128 pixels  
- **Color Mode**: Grayscale  
- **Classes**:
  - `class_0` â†’ Healthy (No Alzheimer's)
  - `class_1` â†’ Early-stage Alzheimer's
  - `class_2` â†’ Moderate Alzheimer's
  - `class_3` â†’ Severe Alzheimer's  

---

## ğŸš€ How to Preprocess the Data
Since the processed dataset is too large to upload, you can generate it yourself by running the preprocessing scripts.

### **1ï¸âƒ£ Install Dependencies**
Ensure you have the required libraries installed:
```bash
pip install -r requirements.txt

2ï¸âƒ£ Run Data Preprocessing
Execute the following script to preprocess and augment the data:
python src/utils/data_preprocessor.py

This script performs:

Rescaling: Normalizes pixel values between [0, 1]
Augmentation: Random rotation, flipping, brightness adjustments
Splitting: 80% Train, 10% Validation, 10% Test
The processed data will be saved in the data/processed/ folder.

ğŸ“Œ Notes
If you wish to use the original dataset directly, you can modify the data pipeline in data_loader.py.
If needed, use Google Drive or DVC (Data Version Control) to store and retrieve large datasets efficiently.

ğŸ›  Troubleshooting
If you run into memory issues while preprocessing, consider reducing batch sizes in the script.
If any file paths are incorrect, update them in src/utils/data_loader.py.

For further details, check the code files in src/utils/. ğŸš€












