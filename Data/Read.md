# 📂 Data Folder

## 📌 Overview
This folder contains the dataset for the **Alzheimer's Detection Model**.  
The dataset consists of **6,460 grayscale MRI brain scans** of size **128×128** pixels, used to classify Alzheimer's disease.

Due to storage limitations, only the **raw dataset** is included here. The **augmented and preprocessed** data is not uploaded but can be generated by following the preprocessing steps in the provided scripts.

---

## 🌍 Dataset Source
You can access the original dataset from the following source:  

📌 **[Dataset Link](https://www.kaggle.com/datasets/geethashashankp/alzheimers-disease-mri-dataset/data)**  

Please refer to the dataset license and terms of use before using it.

---

## 📂 Folder Structure
📂 data/ ├── raw/ # Original dataset (uploaded) │ ├── class_0/ # MRI scans of healthy brains │ ├── class_1/ # MRI scans of early-stage Alzheimer's │ ├── class_2/ # MRI scans of moderate Alzheimer's │ ├── class_3/ # MRI scans of severe Alzheimer's │ ├── processed/ # Preprocessed and augmented data (not uploaded)

## 🔹 Dataset Details
- **Total Images**: 6,460  
- **Image Size**: 128 × 128 pixels  
- **Color Mode**: Grayscale  
- **Classes**:
  - `class_0` → Healthy (No Alzheimer's)
  - `class_1` → Early-stage Alzheimer's
  - `class_2` → Moderate Alzheimer's
  - `class_3` → Severe Alzheimer's  

---

## 🚀 How to Preprocess the Data
Since the processed dataset is too large to upload, you can generate it yourself by running the preprocessing scripts.

### **1️⃣ Install Dependencies**
Ensure you have the required libraries installed:
```bash
pip install -r requirements.txt

2️⃣ Run Data Preprocessing
Execute the following script to preprocess and augment the data:
python src/utils/data_preprocessor.py

This script performs:

Rescaling: Normalizes pixel values between [0, 1]
Augmentation: Random rotation, flipping, brightness adjustments
Splitting: 80% Train, 10% Validation, 10% Test
The processed data will be saved in the data/processed/ folder.

📌 Notes
If you wish to use the original dataset directly, you can modify the data pipeline in data_loader.py.
If needed, use Google Drive or DVC (Data Version Control) to store and retrieve large datasets efficiently.

🛠 Troubleshooting
If you run into memory issues while preprocessing, consider reducing batch sizes in the script.
If any file paths are incorrect, update them in src/utils/data_loader.py.

For further details, check the code files in src/utils/. 🚀












